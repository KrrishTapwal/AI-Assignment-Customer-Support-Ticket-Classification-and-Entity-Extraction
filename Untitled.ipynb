{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc22fc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Sample:\n",
      "   ticket_id                                        ticket_text  \\\n",
      "0          1  Payment issue for my SmartWatch V2. I was unde...   \n",
      "1          2  Can you tell me more about the UltraClean Vacu...   \n",
      "2          3  I ordered SoundWave 300 but got EcoBreeze AC i...   \n",
      "3          4  Facing installation issue with PhotoSnap Cam. ...   \n",
      "4          5  Order #30903 for Vision LED TV is 13 days late...   \n",
      "\n",
      "           issue_type urgency_level            product  \n",
      "0     Billing Problem        Medium      SmartWatch V2  \n",
      "1     General Inquiry           NaN  UltraClean Vacuum  \n",
      "2          Wrong Item        Medium      SoundWave 300  \n",
      "3  Installation Issue           Low      PhotoSnap Cam  \n",
      "4       Late Delivery           NaN      Vision LED TV  \n",
      "\n",
      "Missing Values:\n",
      "ticket_id         0\n",
      "ticket_text      55\n",
      "issue_type       76\n",
      "urgency_level    52\n",
      "product           0\n",
      "dtype: int64\n",
      "\n",
      "Unique Issue Types: 7\n",
      "Issue Type Distribution:\n",
      " issue_type\n",
      "Billing Problem       146\n",
      "General Inquiry       146\n",
      "Account Access        143\n",
      "Installation Issue    142\n",
      "Product Defect        121\n",
      "Wrong Item            114\n",
      "Late Delivery         112\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Urgency Level Distribution:\n",
      " urgency_level\n",
      "High      330\n",
      "Medium    319\n",
      "Low       299\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample Ticket Text:\n",
      " Payment issue for my SmartWatch V2. I was underbilled for order #29224.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_excel(\"ai_dev_assignment_tickets_complex_1000.xls\")\n",
    "\n",
    "# Preview the data\n",
    "print(\"Data Sample:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nUnique Issue Types:\", df['issue_type'].nunique())\n",
    "print(\"Issue Type Distribution:\\n\", df['issue_type'].value_counts())\n",
    "\n",
    "print(\"\\nUrgency Level Distribution:\\n\", df['urgency_level'].value_counts())\n",
    "\n",
    "print(\"\\nSample Ticket Text:\\n\", df['ticket_text'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6734c7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         ticket_text  \\\n",
      "0  Payment issue for my SmartWatch V2. I was unde...   \n",
      "2  I ordered SoundWave 300 but got EcoBreeze AC i...   \n",
      "3  Facing installation issue with PhotoSnap Cam. ...   \n",
      "5  Can you tell me more about the PhotoSnap Cam w...   \n",
      "6   is malfunction. It stopped working after just...   \n",
      "\n",
      "                                          clean_text  \n",
      "0       payment issue smartwatch v underbilled order  \n",
      "2  ordered soundwave got ecobreeze ac instead ord...  \n",
      "3  facing installation issue photosnap cam setup ...  \n",
      "5     tell photosnap cam warranty also available red  \n",
      "6                    malfunction stopped working day  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/krizzkamaliyagmail.com/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/krizzkamaliyagmail.com/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/krizzkamaliyagmail.com/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/var/folders/nl/h32y1g9s3yj85pfnr0n7zqbh0000gn/T/ipykernel_35545/2274702854.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean['clean_text'] = df_clean['ticket_text'].apply(preprocess_text)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK data if not already present\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Drop rows with missing critical fields\n",
    "df_clean = df.dropna(subset=['ticket_text', 'issue_type', 'urgency_level'])\n",
    "\n",
    "# Initialize preprocessor components\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "df_clean['clean_text'] = df_clean['ticket_text'].apply(preprocess_text)\n",
    "\n",
    "# Show sample result\n",
    "print(df_clean[['ticket_text', 'clean_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce7337ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nl/h32y1g9s3yj85pfnr0n7zqbh0000gn/T/ipykernel_35545/2954001139.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean['ticket_length'] = df_clean['clean_text'].apply(lambda x: len(x.split()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ticket_length  sentiment\n",
      "0              6        0.0\n",
      "2              8        0.0\n",
      "3              8       -0.5\n",
      "5              7        0.3\n",
      "6              4        0.0\n",
      "TF-IDF shape: (826, 105)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nl/h32y1g9s3yj85pfnr0n7zqbh0000gn/T/ipykernel_35545/2954001139.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean['sentiment'] = df_clean['ticket_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Vectorize text using TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=1000)  # You can change this number\n",
    "X_tfidf = tfidf.fit_transform(df_clean['clean_text'])\n",
    "\n",
    "# Add ticket length\n",
    "df_clean['ticket_length'] = df_clean['clean_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Add sentiment score\n",
    "df_clean['sentiment'] = df_clean['ticket_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Print sample features\n",
    "print(df_clean[['ticket_length', 'sentiment']].head())\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c26eed02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue Type Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "    Account Access       1.00      1.00      1.00        23\n",
      "   Billing Problem       1.00      1.00      1.00        19\n",
      "   General Inquiry       1.00      1.00      1.00        25\n",
      "Installation Issue       1.00      1.00      1.00        29\n",
      "     Late Delivery       1.00      1.00      1.00        17\n",
      "    Product Defect       1.00      1.00      1.00        30\n",
      "        Wrong Item       1.00      1.00      1.00        23\n",
      "\n",
      "          accuracy                           1.00       166\n",
      "         macro avg       1.00      1.00      1.00       166\n",
      "      weighted avg       1.00      1.00      1.00       166\n",
      "\n",
      "Urgency Level Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        High       0.40      0.35      0.37        66\n",
      "         Low       0.29      0.33      0.31        43\n",
      "      Medium       0.32      0.33      0.32        57\n",
      "\n",
      "    accuracy                           0.34       166\n",
      "   macro avg       0.33      0.34      0.33       166\n",
      "weighted avg       0.34      0.34      0.34       166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Features: Combine TF-IDF with numeric features\n",
    "import numpy as np\n",
    "X_combined = hstack([X_tfidf, \n",
    "                     np.array(df_clean[['ticket_length', 'sentiment']])])\n",
    "\n",
    "# Labels\n",
    "y_issue = df_clean['issue_type']\n",
    "y_urgency = df_clean['urgency_level']\n",
    "\n",
    "# Split datasets\n",
    "X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(X_combined, y_issue, test_size=0.2, random_state=42)\n",
    "X_train_u, X_test_u, y_train_u, y_test_u = train_test_split(X_combined, y_urgency, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Issue Type Classifier\n",
    "model_issue = LogisticRegression(max_iter=1000)\n",
    "model_issue.fit(X_train_i, y_train_i)\n",
    "y_pred_i = model_issue.predict(X_test_i)\n",
    "print(\"Issue Type Classification Report:\\n\", classification_report(y_test_i, y_pred_i))\n",
    "\n",
    "# Train Urgency Level Classifier\n",
    "model_urgency = LogisticRegression(max_iter=1000)\n",
    "model_urgency.fit(X_train_u, y_train_u)\n",
    "y_pred_u = model_urgency.predict(X_test_u)\n",
    "print(\"Urgency Level Classification Report:\\n\", classification_report(y_test_u, y_pred_u))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "217c25d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text: Payment issue for my SmartWatch V2. I was underbilled for order #29224.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'product_list_lower' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m sample_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPayment issue for my SmartWatch V2. I was underbilled for order #29224.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample Text:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sample_text)\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted Entities:\u001b[39m\u001b[38;5;124m\"\u001b[39m, extract_entities(sample_text))\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m, in \u001b[0;36mextract_entities\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     11\u001b[0m text_lower \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Extract products\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m products_found \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m product_list_lower \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m text_lower]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Extract complaint keywords\u001b[39;00m\n\u001b[1;32m     17\u001b[0m complaints_found \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m complaint_keywords \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text_lower]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'product_list_lower' is not defined"
     ]
    }
   ],
   "source": [
    "# Expanded complaint keywords list\n",
    "complaint_keywords = [\n",
    "    \"broken\", \"late\", \"not working\", \"malfunction\", \"error\", \"damaged\", \n",
    "    \"defective\", \"defect\", \"faulty\", \"delay\", \"issue\", \"problem\", \"failed\", \"missing\"\n",
    "]\n",
    "\n",
    "# Refined date regex (more natural phrasing, avoids single digits or order #s)\n",
    "date_pattern = r'\\b(?:\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\\d{4}-\\d{2}-\\d{2}|\\w+\\s+\\d{1,2}(?:st|nd|rd|th)?|\\d+\\s+days?|yesterday|today|tomorrow)\\b'\n",
    "\n",
    "def extract_entities(text):\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Extract products\n",
    "    products_found = [p for p in product_list_lower if p in text_lower]\n",
    "    \n",
    "    # Extract complaint keywords\n",
    "    complaints_found = [word for word in complaint_keywords if word in text_lower]\n",
    "    \n",
    "    # Extract dates\n",
    "    dates_found = re.findall(date_pattern, text_lower)\n",
    "    \n",
    "    return {\n",
    "        \"products\": products_found,\n",
    "        \"complaint_keywords\": complaints_found,\n",
    "        \"dates\": dates_found\n",
    "    }\n",
    "\n",
    "# Test again on same sample\n",
    "sample_text = \"Payment issue for my SmartWatch V2. I was underbilled for order #29224.\"\n",
    "print(\"Sample Text:\", sample_text)\n",
    "print(\"Extracted Entities:\", extract_entities(sample_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110d766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Reinitialize and fit the vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)\n",
    "X = tfidf_vectorizer.fit_transform(df_clean['clean_text'])\n",
    "\n",
    "# Labels\n",
    "y_issue = df_clean['issue_type']\n",
    "y_urgency = df_clean['urgency_level']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d9de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_issue_train, y_issue_test = train_test_split(X, y_issue, test_size=0.2, random_state=42)\n",
    "_, _, y_urgency_train, y_urgency_test = train_test_split(X, y_urgency, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train models\n",
    "issue_model = LogisticRegression(max_iter=1000)\n",
    "issue_model.fit(X_train, y_issue_train)\n",
    "\n",
    "urgency_model = LogisticRegression(max_iter=1000)\n",
    "urgency_model.fit(X_train, y_urgency_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a5a10ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_ticket_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sample_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPayment issue for my SmartWatch V2. I was underbilled for order #29224.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m predict_ticket_info(sample_input, issue_model, urgency_model, tfidf_vectorizer)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_ticket_info' is not defined"
     ]
    }
   ],
   "source": [
    "sample_input = \"Payment issue for my SmartWatch V2. I was underbilled for order #29224.\"\n",
    "result = predict_ticket_info(sample_input, issue_model, urgency_model, tfidf_vectorizer)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21766da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759db4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a98aa29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
